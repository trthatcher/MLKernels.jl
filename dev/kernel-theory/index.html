<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Kernel Theory · MLKernels.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLKernels.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../interface/">Interface</a></li><li><a class="toctext" href="../kernels/">Kernels</a></li><li class="current"><a class="toctext" href>Kernel Theory</a><ul class="internal"><li><a class="toctext" href="#The-Kernel-Trick-1">The Kernel Trick</a></li><li><a class="toctext" href="#Kernels-1">Kernels</a></li><li><a class="toctext" href="#Further-Reading-1">Further Reading</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Kernel Theory</a></li></ul><a class="edit-page" href="https://github.com/trthatcher/MLKernels.jl/blob/master/docs/src/kernel-theory.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Kernel Theory</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Kernel-Theory-1" href="#Kernel-Theory-1">Kernel Theory</a></h1><h2><a class="nav-anchor" id="The-Kernel-Trick-1" href="#The-Kernel-Trick-1">The Kernel Trick</a></h2><p>Many machine and statistical learning algorithms, such as support vector machines and  principal components analysis, are based on <strong>inner products</strong>. These methods can often be  generalized through use of the <strong>kernel trick</strong> to create anonlinear decision boundary  without using an explicit mapping to another space. </p><p>The kernel trick makes use of <strong>Mercer kernels</strong> which operate on vectors in the input  space but can be expressed as inner products in another space. In other words, if  <span>$\mathcal{X}$</span> is the input vector space and <span>$\kappa$</span> is the Mercer kernel function,  then for some vector space <span>$\mathcal{V}$</span> there exists a function <code>\phi</code> such that:</p><div>\[\kappa(x_1, x_2) 
= \left\langle \phi(x_1), \phi(x_2)\right\rangle_{\mathcal{V}}
\qquad x_1, x_2 \in \mathcal{X}\]</div><p>In machine learning, the vector space <span>$\mathcal{X}$</span> is known as the feature space and the  function <span>$\phi$</span> is known as a feature map. A simple example of a feature map can be shown  with the Polynomial Kernel:</p><div>\[\kappa(\mathbf{x},\mathbf{y}) = (a\mathbf{x}^\intercal\mathbf{y} + c)^{d}
\qquad \mathbf{x},\mathbf{y} \in \mathbb{R}^n, 
\quad a, c \in \mathbb{R}_+
\quad d \in \mathbb{Z}_+\]</div><p>In our example, we will use <span>$n=2$</span>, <span>$d=2$</span>, <span>$a=1$</span> and <span>$c=0$</span>. Substituting these  values in, we get the following kernel function:</p><div>\[\kappa(\mathbf{x},\mathbf{y}) = \left(x_1 y_1 + x_2 y_2\right)^2
= x_1^2 y_1^2 + x_1 x_2 y_1 y_2 + x_2^2 y_2^2
= \phi(\mathbf{x})^\intercal\phi(\mathbf{y})\]</div><p>Where the feature map <span>$\phi : \mathbb{R}^2 \rightarrow \mathbb{R}^3$</span> is defined by:</p><div>\[\phi(\mathbf{x}) = 
\begin{bmatrix}
    x_1^2 \\
    x_1 x_2 \\
    x_2^2
\end{bmatrix}\]</div><p>The advantage of the implicit feature map is that we may transform non-linearly data into  linearly separable data in the implicit space.</p><h2><a class="nav-anchor" id="Kernels-1" href="#Kernels-1">Kernels</a></h2><p>The kernel methods are a class of algorithms that are used for pattern analysis. These  methods make use of <strong>kernel</strong> functions. A symmetric, real valued kernel function  <span>$\kappa: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$</span> is said to be <strong>positive  definite</strong> or <strong>Mercer</strong> if and only:</p><div>\[\sum_{i=1}^n \sum_{j=1}^n c_i c_j \kappa(\mathbf{x}_i,\mathbf{x}_j) \geq 0\]</div><p>for all <span>$n \in \mathbb{N}$</span>, <span>$\{\mathbf{x}_1, \dots, \mathbf{x}_n\} \subseteq \mathcal{X}$</span> and <span>$\{c_1, \dots, c_n\} \subseteq \mathbb{R}$</span>. Similarly, a real valued kernel function is said to be <strong>negative definite</strong> if and only if:</p><div>\[\sum_{i=1}^n \sum_{j=1}^n c_i c_j \kappa(\mathbf{x}_i,\mathbf{x}_j) \leq 0 \qquad \sum_{i=1}^n c_i = 0\]</div><p>for <span>$n \geq 2$</span>, <span>$\{\mathbf{x}_1, \dots, \mathbf{x}_n\} \subseteq \mathcal{X}$</span> and  <span>$\{c_1, \dots, c_n\} \subseteq \mathbb{R}$</span>. In machine learning literature,  <strong>conditionally positive definite</strong> kernels are often studied instead. This is simply a  reversal of the above inequality. Trivially, every negative definite kernel can be  transformed into a conditionally positive definite kernel by negation.</p><h2><a class="nav-anchor" id="Further-Reading-1" href="#Further-Reading-1">Further Reading</a></h2><ul><li>Berg C, Christensen JPR, Ressel P. 1984. <em>Harmonic Analysis on Semigroups</em>. Springer-Verlag New York. Chapter 3, General Results on Positive and Negative Definite Matrices and Kernels; p. 66-85.</li><li>Bouboulis P. 2014. <em>Academic Press Library in Signal Processing, Volume 1: Array and Statistical Signal Processing (1st ed.)</em>. Academic Press. Chapter 17, Online Learning in Reproducing Kernel Hilbert Spaces; p. 883-987.</li><li>Genton M.G. 2002. <em>Classes of kernels for machine learning: a statistics perspective</em>. The Journal of Machine Learning Research. Volume 2 (March 2002), 299-312.</li><li>Rasmussen C, Williams CKI. 2005. <em>Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)</em>. The MIT Press. Chapter 4, Covariance Functions; p. 79-104.</li></ul><footer><hr/><a class="previous" href="../kernels/"><span class="direction">Previous</span><span class="title">Kernels</span></a></footer></article></body></html>
