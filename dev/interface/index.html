<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Interface · MLKernels.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLKernels.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Interface</a><ul class="internal"><li><a class="toctext" href="#Data-Orientation-1">Data Orientation</a></li><li><a class="toctext" href="#Essentials-1">Essentials</a></li><li><a class="toctext" href="#Approximation-1">Approximation</a></li></ul></li><li><a class="toctext" href="../kernels/">Kernels</a></li><li><a class="toctext" href="../kernel-theory/">Kernel Theory</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Interface</a></li></ul><a class="edit-page" href="https://github.com/trthatcher/MLKernels.jl/blob/master/docs/src/interface.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Interface</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Interface-1" href="#Interface-1">Interface</a></h1><h2><a class="nav-anchor" id="Data-Orientation-1" href="#Data-Orientation-1">Data Orientation</a></h2><p>Data matrices may be oriented in one of two ways with respect to the observations. Functions producing a kernel matrix require an <code>orient</code> argument to specify the orientation of the observations within the provided data matrix.</p><h3><a class="nav-anchor" id="Row-Orientation-(Default)-1" href="#Row-Orientation-(Default)-1">Row Orientation (Default)</a></h3><p>An orientation of <code>Val(:row)</code> identifies when observation vector corresponds to a row of the data matrix. This is commonly used in the field of statistics in the context of <a href="https://en.wikipedia.org/wiki/Design_matrix">design matrices</a>.</p><p>For example, for data matrix <span>$\mathbf{X}$</span> consisting of observations <span>$\mathbf{x}_1$</span>, <span>$\mathbf{x}_2$</span>, <span>$\ldots$</span>, <span>$\mathbf{x}_n$</span>:</p><div>\[\mathbf{X}_{row} =
\begin{bmatrix}
    \leftarrow \mathbf{x}_1 \rightarrow \\
    \leftarrow \mathbf{x}_2 \rightarrow \\
    \vdots \\
    \leftarrow \mathbf{x}_n \rightarrow
\end{bmatrix}\]</div><p>When row-major ordering is used, then the kernel matrix of <span>$\mathbf{X}$</span> will match the dimensions of <span>$\mathbf{X}^{\intercal}\mathbf{X}$</span>. Similarly, the kernel matrix will match the dimension of <span>$\mathbf{X}^{\intercal}\mathbf{Y}$</span> for row-major ordering of data matrix <span>$\mathbf{X}$</span> and <span>$\mathbf{Y}$</span>.</p><h3><a class="nav-anchor" id="Column-Orientation-1" href="#Column-Orientation-1">Column Orientation</a></h3><p>An orientation of <code>Val(:col)</code> identifies when each observation vector corresponds to a column of the data matrix:</p><div>\[\mathbf{X}_{col} =
\mathbf{X}_{row}^{\intercal} =
\begin{bmatrix}
    \uparrow &amp; \uparrow &amp; &amp; \uparrow  \\
    \mathbf{x}_1 &amp; \mathbf{x}_2 &amp; \cdots &amp; \mathbf{x_n} \\
    \downarrow &amp; \downarrow &amp; &amp; \downarrow
\end{bmatrix}\]</div><p>With column-major ordering, the kernel matrix will match the dimensions of <span>$\mathbf{XX}^{\intercal}$</span>. Similarly, the kernel matrix of data matrices <span>$\mathbf{X}$</span> and <span>$\mathbf{Y}$</span> match the dimensions of <span>$\mathbf{XY}^{\intercal}$</span>.</p><h2><a class="nav-anchor" id="Essentials-1" href="#Essentials-1">Essentials</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.ismercer-Tuple{Kernel}" href="#MLKernels.ismercer-Tuple{Kernel}"><code>MLKernels.ismercer</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">ismercer(κ::Kernel)</code></pre><p>Returns <code>true</code> if kernel <code>κ</code> is a Mercer kernel; <code>false</code> otherwise.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelfunctions.jl#L18-L22">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.isnegdef-Tuple{Kernel}" href="#MLKernels.isnegdef-Tuple{Kernel}"><code>MLKernels.isnegdef</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">isnegdef(κ::Kernel)</code></pre><p>Returns <code>true</code> if the kernel <code>κ</code> is a negative definite kernel; <code>false</code> otherwise.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelfunctions.jl#L25-L29">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.isstationary-Tuple{Kernel}" href="#MLKernels.isstationary-Tuple{Kernel}"><code>MLKernels.isstationary</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">isstationary(κ::Kernel)</code></pre><p>Returns <code>true</code> if the kernel <code>κ</code> is a stationary kernel; <code>false</code> otherwise.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelfunctions.jl#L32-L36">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.isisotropic-Tuple{Kernel}" href="#MLKernels.isisotropic-Tuple{Kernel}"><code>MLKernels.isisotropic</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">isisotropic(κ::Kernel)</code></pre><p>Returns <code>true</code> if the kernel <code>κ</code> is an isotropic kernel; <code>false</code> otherwise.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelfunctions.jl#L39-L43">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.kernel-Union{Tuple{T}, Tuple{Kernel{T},Real,Real}} where T" href="#MLKernels.kernel-Union{Tuple{T}, Tuple{Kernel{T},Real,Real}} where T"><code>MLKernels.kernel</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">kernel(κ::Kernel, x, y)</code></pre><p>Apply the kernel <code>κ</code> to <span>$x$</span> and <span>$y$</span> where <span>$x$</span> and <span>$y$</span> are vectors or scalars of some subtype of <span>$Real$</span>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelmatrix.jl#L96-L101">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.Orientation" href="#MLKernels.Orientation"><code>MLKernels.Orientation</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><pre><code class="language-none">Orientation</code></pre><p>Union of the two <code>Val</code> types representing the data matrix orientations:</p><ol><li><code>Val{:row}</code> identifies when observation vector corresponds to a row of the data matrix</li><li><code>Val{:col}</code> identifies when each observation vector corresponds to a column of the data matrix</li></ol></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/MLKernels.jl#L59-L67">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.kernelmatrix-Union{Tuple{T1}, Tuple{T}, Tuple{Union{Val{:row}, Val{:col}},Kernel{T},AbstractArray{T1,2},Bool}} where T1 where T" href="#MLKernels.kernelmatrix-Union{Tuple{T1}, Tuple{T}, Tuple{Union{Val{:row}, Val{:col}},Kernel{T},AbstractArray{T1,2},Bool}} where T1 where T"><code>MLKernels.kernelmatrix</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">kernelmatrix([σ::Orientation,] κ::Kernel, X::Matrix [, symmetrize::Bool])</code></pre><p>Calculate the kernel matrix of <code>X</code> with respect to kernel <code>κ</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelmatrix.jl#L114-L118">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.kernelmatrix!-Union{Tuple{T}, Tuple{Union{Val{:row}, Val{:col}},Array{T,2},Kernel{T},AbstractArray{T,2},Bool}} where T&lt;:AbstractFloat" href="#MLKernels.kernelmatrix!-Union{Tuple{T}, Tuple{Union{Val{:row}, Val{:col}},Array{T,2},Kernel{T},AbstractArray{T,2},Bool}} where T&lt;:AbstractFloat"><code>MLKernels.kernelmatrix!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">kernelmatrix!(σ::Orientation, K::Matrix, κ::Kernel, X::Matrix, symmetrize::Bool)</code></pre><p>In-place version of <code>kernelmatrix</code> where pre-allocated matrix <code>K</code> will be overwritten with the kernel matrix.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelmatrix.jl#L39-L44">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.kernelmatrix-Union{Tuple{T2}, Tuple{T1}, Tuple{T}, Tuple{Union{Val{:row}, Val{:col}},Kernel{T},AbstractArray{T1,2},AbstractArray{T2,2}}} where T2 where T1 where T" href="#MLKernels.kernelmatrix-Union{Tuple{T2}, Tuple{T1}, Tuple{T}, Tuple{Union{Val{:row}, Val{:col}},Kernel{T},AbstractArray{T1,2},AbstractArray{T2,2}}} where T2 where T1 where T"><code>MLKernels.kernelmatrix</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">kernelmatrix([σ::Orientation,] κ::Kernel, X::Matrix, Y::Matrix)</code></pre><p>Calculate the base matrix of <code>X</code> and <code>Y</code> with respect to kernel <code>κ</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelmatrix.jl#L137-L141">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.kernelmatrix!-Union{Tuple{T}, Tuple{Union{Val{:row}, Val{:col}},Array{T,2},Kernel{T},AbstractArray{T,2},AbstractArray{T,2}}} where T&lt;:AbstractFloat" href="#MLKernels.kernelmatrix!-Union{Tuple{T}, Tuple{Union{Val{:row}, Val{:col}},Array{T,2},Kernel{T},AbstractArray{T,2},AbstractArray{T,2}}} where T&lt;:AbstractFloat"><code>MLKernels.kernelmatrix!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">kernelmatrix!(σ::Orientation, K::Matrix, κ::Kernel, X::Matrix, Y::Matrix)</code></pre><p>In-place version of <code>kernelmatrix</code> where pre-allocated matrix <code>K</code> will be overwritten with the kernel matrix.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelmatrix.jl#L56-L61">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.centerkernelmatrix!-Union{Tuple{Array{T,2}}, Tuple{T}} where T&lt;:AbstractFloat" href="#MLKernels.centerkernelmatrix!-Union{Tuple{Array{T,2}}, Tuple{T}} where T&lt;:AbstractFloat"><code>MLKernels.centerkernelmatrix!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">centerkernelmatrix(K::Matrix)</code></pre><p>Centers the (rectangular) kernel matrix <code>K</code> with respect to the implicit Kernel Hilbert Space according to the following formula:</p><div>\[[\mathbf{K}]_{ij}
= \langle\phi(\mathbf{x}_i) -\mathbf{\mu}_{\phi\mathbf{x}}, \phi(\mathbf{y}_j)
- \mathbf{\mu}_{\phi\mathbf{y}} \rangle\]</div><p>Where <span>$\mathbf{\mu}_{\phi\mathbf{x}}$</span> and <span>$\mathbf{\mu}_{\phi\mathbf{x}}$</span> are given by:</p><div>\[\mathbf{\mu}_{\phi\mathbf{x}}
= \frac{1}{n} \sum_{i=1}^n \phi(\mathbf{x}_i)
\qquad \qquad
\mathbf{\mu}_{\phi\mathbf{y}}
= \frac{1}{m} \sum_{i=1}^m \phi(\mathbf{y}_i)\]</div></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/kernelmatrix.jl#L164-L184">source</a></section><h2><a class="nav-anchor" id="Approximation-1" href="#Approximation-1">Approximation</a></h2><p>In many cases, fast, approximate results is more important than a perfect result. The Nystrom method can be used to generate a factorization that can be used to approximate a large, symmetric kernel matrix. Given data matrix <span>$\mathbf{X} \in \mathbb{R}^{n \times p}$</span> (one observation per row) and kernel matrix <span>$\mathbf{K} \in \mathbb{R}^{n \times n}$</span>, the Nystrom method takes a sample <span>$S$</span> of the observations of <span>$\mathbf{X}$</span> of size <span>$s &lt; n$</span> and generates a factorization such that:</p><div>\[\mathbf{K} \approx \mathbf{C}^{\intercal}\mathbf{WC}\]</div><p>Where <span>$\mathbf{W}$</span> is the <span>$s \times s$</span> pseudo-inverse of the sample kernel matrix based on <span>$S$</span> and <span>$\mathbf{C}$</span> is a <span>$s \times n$</span> matrix.</p><p>The Nystrom method uses an eigendecomposition of the sample kernel matrix of <span>$\mathbf{X}$</span> to estimate <span>$\mathbf{K}$</span>. Generally, the order of <span>$\mathbf{K}$</span> must be quite large and the sampling ratio small (ex. 15% or less) for the cost of the computing the full kernel matrix to exceed that of the eigendecomposition. This method will be more effective for kernels that are not a direct function of the dot product as they are not able to make use of BLAS in computing the full matrix <span>$\mathbf{K}$</span> and the cross-over point will occur for smaller <span>$\mathbf{K}$</span>.</p><p><strong>MLKernels.jl</strong> implements the Nystrom approximation:</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.NystromFact" href="#MLKernels.NystromFact"><code>MLKernels.NystromFact</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">NystromFact</code></pre><p>Type for storing a Nystrom factorization. The factorization contains two fields: <code>W</code> and <code>C</code> as described in the <code>nystrom</code> documentation.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/nystrom.jl#L59-L64">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.nystrom" href="#MLKernels.nystrom"><code>MLKernels.nystrom</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">nystrom([σ::Orientation,] κ::Kernel, X::Matrix, [S::Vector])</code></pre><p>Computes a factorization of Nystrom approximation of the square kernel matrix of data matrix <code>X</code> with respect to kernel <code>κ</code>. Returns a <code>NystromFact</code> struct which stores a Nystrom factorization satisfying:</p><div>\[\mathbf{K} \approx \mathbf{C}^{\intercal}\mathbf{WC}\]</div></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/nystrom.jl#L70-L80">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLKernels.kernelmatrix-Union{Tuple{NystromFact{T}}, Tuple{T}} where T&lt;:Union{Float32, Float64}" href="#MLKernels.kernelmatrix-Union{Tuple{NystromFact{T}}, Tuple{T}} where T&lt;:Union{Float32, Float64}"><code>MLKernels.kernelmatrix</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">nystrom(CᵀWC::NystromFact)</code></pre><p>Compute the approximate kernel matrix based on the Nystrom factorization.</p></div></div><a class="source-link" target="_blank" href="https://github.com/trthatcher/MLKernels.jl/blob/85cc602338a9c207ef8e6496f207d767174e4b61/src/nystrom.jl#L100-L104">source</a></section><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Home</span></a><a class="next" href="../kernels/"><span class="direction">Next</span><span class="title">Kernels</span></a></footer></article></body></html>
